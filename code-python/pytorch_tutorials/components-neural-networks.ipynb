{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40698c58",
   "metadata": {},
   "source": [
    "# PyTorch Neural Networks Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f74e8",
   "metadata": {},
   "source": [
    "Building blocks of deep learning models from simple to complex NN \n",
    "to define, train and optimize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b193bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Breakdown of simple Neural Network \n",
    "\n",
    "X  : input \n",
    "Wi : weights i\n",
    "bi : bias i\n",
    "A  : Activation function (provide non linarity)\n",
    "Y  : output\n",
    "\n",
    "To create a forward propagation path \n",
    "\n",
    "( Two layer with one neuron )\n",
    "Z = A(W1.X + b1)\n",
    "Z' = A(Z)\n",
    "Y = W2.Z' + b2\n",
    "\n",
    "Once process of forward propagation finish... \n",
    "\n",
    "Loss function: to optimize the weights\n",
    "Backpropagation : To compute the gradients \n",
    "Optimizer: to compute the new weights \n",
    "\n",
    "Model Input, weights and bias, activation .. for forward propagation flow giving Y\n",
    "Then compute Loss as numerical value, compute gradients by backpropagation and optimizer to change weights\n",
    "\n",
    "'''\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd56b6ea",
   "metadata": {},
   "source": [
    "## Components of PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a48400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for defining custom models (architecture): torch.nn.Module\n",
    "\n",
    "# To create fully connected layers and create connection, Fully connected (dense) layers : torch.nn.Linear\n",
    "\n",
    "# Activation function (here focus on ReLU) : torch.nn.ReLU\n",
    "\n",
    "# Optimizers : torch.optim (with lot of different optiomizers)\n",
    "\n",
    "# Loss function: torch.nn.CrossEntropyLoss (example of possible loss)\n",
    "\n",
    "# To load data in batches, usefull when working with visual data, you load in batches: torch.utils.data.DataLoader\n",
    "# (DataLoader helps to load data efficiently, such as in batches or in GPU)\n",
    "\n",
    "# .... There are different ways of creating a NN \n",
    "# 1. Function : (but harder to interpret)\n",
    "# flexible way of building NN by directly applying operations on tensors (this allow complex architecture and custom operation)\n",
    "# 2. Sequential : (simpler to interpret)\n",
    "# More structured approach, layers stacked in linear order in a continual call nn.Sequential\n",
    "# Used to define basic models with clear layers progression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf1a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a NN \n",
    "\n",
    "import torch.nn as nn # allow to acces the above mentioned components \n",
    "import torch.optim as optim # optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ad526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN with Functional API \n",
    "\n",
    "# Whenever creating a new PyTorch custom model it must inherit from nn.Module\n",
    "class SimpleNN(nn.Module):\n",
    "    # In init we will initialize the layers\n",
    "    # input_size define the number of neurons in the first input layer \n",
    "    # hidden_size is the number of neurons in the intermediate layer \n",
    "    # output_size finally refers to the number of neurons in the final layer before output \n",
    "    # in pytorch when calling nn.Linear the shape of input,ouput of the current fully connected layer is defined \n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__() \n",
    "        # Initializer of the super class nn.Module, ensuring init of it is called \n",
    "\n",
    "        # First just creating the components, not the architecture \n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # fully connected layer 1\n",
    "\n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # fully connected layer 2\n",
    "\n",
    "\n",
    "    # In a flow (forward propagation) we will go as X -> self.fc1(X) -> Z = self.relu -> self.fc2(Z) -> Y\n",
    "    # The pipeline must be defined for the forward propagation \n",
    "    def forward(self, x): # x mus be of input_size \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # I override x during the forward process\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# With this functional API I can define the architecture as I want, and then call it in the forward pass, I'm noty constrained by an order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19125508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN with Sequenctial API \n",
    "\n",
    "class SimpleNNSequential(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__() \n",
    "\n",
    "        # In sequential you don't define the single components inside of the init()\n",
    "        # A pipeline is defined from simple components \n",
    "        # define in a sequence... The flow is constrained here in Sequential()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        # The components are a single sequential object\n",
    "\n",
    "\n",
    "    def forward(self, x): # x mus be of input_size \n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "112a9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When working with CNN or others complex architectures, not always sequenctial flow is used...\n",
    "# sometimes additional flexibility is required, with output skipping layers or reusing inputs in next layers...\n",
    "# Combination of multiple output as input of next layers or other operations are possible...\n",
    "# For example ResNet rely on residual block, where sequential cannot be used !\n",
    "\n",
    "# I require separate components to be used properly inside of the network definition \n",
    "# Use sequential for very simple architecture, but not when complex variables pass is required...\n",
    "\n",
    "# Try to use the network for dummy training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c902b964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# First create dummy data\n",
    "\n",
    "model_func = SimpleNN(input_size=4, hidden_size=8, output_size=3)\n",
    "print(model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ce6c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for training \n",
    "X = torch.randn(10, 4) # 10 samples with 4 features per sample \n",
    "Y = torch.randint(0,3, (10,)) # return 10 random output as label between 0,1,2 (class label)\n",
    "\n",
    "# Compute the loss of the network\n",
    "# CrossEntropyLoss uses softmax activation as last layer, pytorch handle automically this final activation\n",
    "# softmax on output of the nn is added\n",
    "criterion = nn.CrossEntropyLoss() # criterion of training \n",
    "\n",
    "optimizer = optim.Adam(model_func.parameters(), lr=0.01) # The Adam optimizer work on the NN model, it must optimize the model parameters with learning rate 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "652d2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset X:  tensor([[-0.3689,  1.4739,  0.4202, -0.9198],\n",
      "        [ 0.8597,  1.2753, -1.5125,  1.8800],\n",
      "        [-1.0656, -0.5052,  1.0023, -1.2221],\n",
      "        [-0.5868,  0.3564,  1.6431, -0.6329],\n",
      "        [ 0.5844,  2.1104,  1.0745, -0.3469],\n",
      "        [-0.4041, -0.6631,  0.4523,  0.1185],\n",
      "        [-0.0886,  0.6753,  1.2095,  0.4182],\n",
      "        [-0.3485,  0.1890, -0.0694,  0.3447],\n",
      "        [ 0.9207, -1.2917,  0.0984, -1.6139],\n",
      "        [ 1.0118, -0.3596, -0.7790,  0.1209]])\n",
      "Dataset Y:  tensor([2, 1, 1, 2, 0, 1, 0, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset X: \", X)\n",
    "print(\"Dataset Y: \", Y)\n",
    "\n",
    "# Remember that even if in the NN model the output has been specified as output_size=3, then a softmax is applyed on it\n",
    "# the output neuron predict each label of the classifier, than softmax select just one class (maximum probability )\n",
    "# Each neuron of output layer is responsible for each class \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155d08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10]/50, Loss : 0.2576\n",
      "Epoch [20]/50, Loss : 0.1688\n",
      "Epoch [30]/50, Loss : 0.1104\n",
      "Epoch [40]/50, Loss : 0.0754\n",
      "Epoch [50]/50, Loss : 0.0543\n",
      "Epoch [60]/50, Loss : 0.0410\n",
      "Epoch [70]/50, Loss : 0.0323\n",
      "Epoch [80]/50, Loss : 0.0264\n",
      "Epoch [90]/50, Loss : 0.0220\n",
      "Epoch [100]/50, Loss : 0.0188\n",
      "Epoch [110]/50, Loss : 0.0162\n",
      "Epoch [120]/50, Loss : 0.0142\n",
      "Epoch [130]/50, Loss : 0.0126\n",
      "Epoch [140]/50, Loss : 0.0112\n",
      "Epoch [150]/50, Loss : 0.0101\n"
     ]
    }
   ],
   "source": [
    "# Define the training law\n",
    "\n",
    "epoch = 150\n",
    "\n",
    "for e in range(epoch):\n",
    "    # first clear the gradient stored on the model \n",
    "    optimizer.zero_grad() # tell the optimizer to clear the gradients\n",
    "    outputs = model_func(X) # pass all values to the model\n",
    "    # compute the loss\n",
    "    loss = criterion(outputs, Y) # minimize loss from real values Y with respect to predicted outputs\n",
    "\n",
    "    # after computing loss, the backpropagation is performed, calculating gradients \n",
    "    loss.backward() # it compute gradients by backward propagation applied on loss itself\n",
    "    # once gradient are being computed, optimizer store it and perfrom theoptimization step \n",
    "    optimizer.step() # update all the weights based on the loss\n",
    "\n",
    "    if (e + 1) % 10 == 0: # print current loss every 10th iteration \n",
    "        print(f\"Epoch [{e+1}]/50, Loss : {loss.item() :.4f}\")\n",
    "\n",
    "# The model learn from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f14f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
