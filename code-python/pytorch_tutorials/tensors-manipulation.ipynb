{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa431d6c",
   "metadata": {},
   "source": [
    "# Tensors Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805096d",
   "metadata": {},
   "source": [
    "Help to operate with tensors in a better way for efficient workflows. \n",
    "\n",
    "We refer to techniques/operations to alter the structure shape and content of tensors. \n",
    "\n",
    "e.g: \n",
    "- Reshaping \n",
    "- Slicing\n",
    "- Joining multiple tensors \n",
    "- Splitting a single tensor in multiple sections \n",
    "- Transposing tensor (NxM to MxN transposition)\n",
    "- Permuting Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee53793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8db47",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44dce10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping tensors\n",
    "# We can rely on two methods: reshape, view \n",
    "# reshape and view have their own concept important to understand in depth ... \n",
    "\n",
    "original_tensor = torch.arange(12) # generate a tensor in range 0-11\n",
    "print(original_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f674f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor : \n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Length of tensor :  12\n",
      "Dimension of tensor :  1\n",
      "\n",
      "Rehsaped Tensor : \n",
      " tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      " New tensor Dimension :  2\n",
      " New tensor Elements :  12\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tensor : \\n\", original_tensor)\n",
    "# using reshape\n",
    "print(\"Length of tensor : \", original_tensor.nelement())\n",
    "\n",
    "# based on its length, 12 elements, I can reshape the vector\n",
    "print(\"Dimension of tensor : \", original_tensor.ndim)\n",
    "\n",
    "# To convert it to a matrix: \n",
    "reshaped_tensor = original_tensor.reshape(2,6) # I specify the new shape \n",
    "print(\"\\nRehsaped Tensor : \\n\", reshaped_tensor)\n",
    "\n",
    "print(\" New tensor Dimension : \", reshaped_tensor.ndim)\n",
    "print(\" New tensor Elements : \", reshaped_tensor.nelement()) # You obviously keep the n of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rehsaped Tensor : \n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      " New tensor Dimension :  2\n",
      " New tensor Elements :  12\n"
     ]
    }
   ],
   "source": [
    "# To convert it to a different shape \n",
    "reshaped_tensor = original_tensor.reshape(3,4) # I specify the new shape \n",
    "print(\"\\nRehsaped Tensor : \\n\", reshaped_tensor)\n",
    "\n",
    "print(\" New tensor Dimension : \", reshaped_tensor.ndim)\n",
    "print(\" New tensor Elements : \", reshaped_tensor.nelement()) # You obviously keep the n of elements\n",
    "\n",
    "# Still  keeping consistent dimension when reshaping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa78913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rehsaped Tensor : \n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      " New tensor Dimension :  1\n",
      " New tensor Elements :  12\n"
     ]
    }
   ],
   "source": [
    "# For reshaping, view allow to do the same but internally it changes crucially the memory behavior \n",
    "# Internally view requires a contiguous memory (in the RAM all datapoint in view are on a sequence)\n",
    "\n",
    "# If original_tensor is not stored contiguously in memory, view() return an error\n",
    "flatten_tensor = original_tensor.view(-1) # -1 is used to automatic calculation of dimension, with -1 it keep one dimension \n",
    "\n",
    "print(\"\\nRehsaped Tensor : \\n\", flatten_tensor)\n",
    "\n",
    "print(\" New tensor Dimension : \", flatten_tensor.ndim)\n",
    "print(\" New tensor Elements : \", flatten_tensor.nelement()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f031f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor is contiguous ?  True\n"
     ]
    }
   ],
   "source": [
    "## To check contiguity of original tensor: \n",
    "# stored in a sequential memory location\n",
    "print(\"Original tensor is contiguous ? \" , original_tensor.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de54682",
   "metadata": {},
   "source": [
    "## Slicing\n",
    "Extract specific portions of tensors using slicing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "462dfb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor : \n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Tensor Dimension:  2\n",
      "Tensor Shape:  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Original Tensor : \\n\", tensor_a)\n",
    "print(\"Tensor Dimension: \", tensor_a.ndim)\n",
    "print(\"Tensor Shape: \", tensor_a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "350860a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Row    :  tensor([1, 2, 3])\n",
      "First Column :  tensor([1, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# extract first row \n",
    "print(\"First Row    : \", tensor_a[0])\n",
    "print(\"First Column : \", tensor_a[:, 0])\n",
    "# Same indexing behavior already explored before and used in lists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ba6745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced Tensor :  tensor([[5, 6],\n",
      "        [8, 9]])\n",
      "Dim :  torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Extract 2x2 matrix \n",
    "sub_tensor = tensor_a[1:, 1:]\n",
    "print(\"Sliced Tensor : \", sub_tensor)\n",
    "print(\"Dim : \", sub_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc02bd",
   "metadata": {},
   "source": [
    "## Joining Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a668b704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A : \n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Tensor B : \n",
      " tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "Concatenate on rows : \n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "Concatenate on columns : \n",
      " tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Two type of joining \n",
    "# 1. torch.cat() : try to merge tensors along an existing dimension \n",
    "\n",
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Create new tensor with torch.cat() mering along existing dimension\n",
    "# the order of the specified tensor to concatenate is relevant \n",
    "concat_tensor_rows = torch.cat((tensor_a, tensor_b), dim=0) # dim=0 cat aloong rows \n",
    "concat_tensor_cols = torch.cat((tensor_a, tensor_b), dim=1) # dim=1 cat aloong columns \n",
    "\n",
    "print(\"Tensor A : \\n\", tensor_a)\n",
    "print(\"Tensor B : \\n\", tensor_b)\n",
    "\n",
    "print(\"Concatenate on rows : \\n\", concat_tensor_rows)\n",
    "print(\"Concatenate on columns : \\n\", concat_tensor_cols)\n",
    "\n",
    "# We are not creating new dimensions, the ndim is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A : \n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Tensor B : \n",
      " tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "stack on rows shape:  torch.Size([2, 2, 2])\n",
      "stack on rows : \n",
      " tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "stack on cols shape:  torch.Size([2, 2, 2])\n",
      "stack on columns : \n",
      " tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# 2. stack() : creates a new dimension which increases the tensor's rank \n",
    "stack_tensor_rows = torch.stack((tensor_a, tensor_b), dim=0) # dim=0 stack aloong rows \n",
    "stack_tensor_cols = torch.stack((tensor_a, tensor_b), dim=1) # dim=1 stack aloong columns \n",
    "\n",
    "print(\"Tensor A : \\n\", tensor_a)\n",
    "print(\"Tensor B : \\n\", tensor_b)\n",
    "\n",
    "print(\"stack on rows shape: \", stack_tensor_rows.shape)\n",
    "print(\"stack on rows : \\n\", stack_tensor_rows)\n",
    "\n",
    "print(\"stack on cols shape: \", stack_tensor_cols.shape)\n",
    "print(\"stack on columns : \\n\", stack_tensor_cols)\n",
    "\n",
    "# It stacks the elemnt appending over rows or columns\n",
    "# stacking increasing the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76ab0cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "torch.stack((tensor1, tensor2), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e958d0",
   "metadata": {},
   "source": [
    "## Splitting Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "964b5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([ 9, 10, 11]))\n",
      "<class 'tuple'>\n",
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5])\n",
      "tensor([6, 7, 8])\n",
      "tensor([ 9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# splitting tensors into smaller chanks \n",
    "# 1. torch.chunk() : divide your tensor into equal sized chunks\n",
    "# 2. torch.split() : allow uneven splitting based on size of tensor \n",
    "\n",
    "# chunk\n",
    "original_tensor = torch.arange(12)\n",
    "\n",
    "chunks = torch.chunk(original_tensor, 5, dim=0) # perform chunk on rows in chunk of 3\n",
    "\n",
    "# even using 5, it find 4 elemnts since it uses only the maximum divisible in equally sized chunks \n",
    "\n",
    "# it return an iterable as a tuple \n",
    "print(chunks)\n",
    "print(type(chunks))\n",
    "\n",
    "# access as usual \n",
    "# print(chunks[0])\n",
    "\n",
    "for chunk in chunks: \n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff615524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]), tensor([10, 11]))\n",
      "<class 'tuple'>\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([10, 11])\n"
     ]
    }
   ],
   "source": [
    "# chunk doesn't give freedom on splitting. \n",
    "# Using split it is allowed uneven splitted (chunk is useful since automatically adapt)\n",
    "\n",
    "# split\n",
    "original_tensor = torch.arange(12)\n",
    "\n",
    "splits = torch.split(original_tensor, 5, dim=0) # perform chunk on rows in chunk of 5 elements\n",
    "# in splits, you specify the ideal number of elements per splits\n",
    "# allowing for uneven splitting \n",
    "\n",
    "# it return an iterable as a tuple \n",
    "print(splits)\n",
    "print(type(splits))\n",
    "\n",
    "# access as usual \n",
    "# print(chunks[0])\n",
    "\n",
    "for split in splits: \n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1310ce",
   "metadata": {},
   "source": [
    "## Transposion and Permutation\n",
    "\n",
    "transpose() : swap two dimension. e.g from MxN you get NxM matrix \n",
    "\n",
    "permute()   : rearranges all dimensions in the specified order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3959552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : \n",
      " tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11],\n",
      "        [12, 13],\n",
      "        [14, 15],\n",
      "        [16, 17],\n",
      "        [18, 19],\n",
      "        [20, 21],\n",
      "        [22, 23]])\n",
      "Original Shape :  torch.Size([12, 2])\n",
      "Transposed : \n",
      " tensor([[ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22],\n",
      "        [ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23]])\n",
      "Transposed Shape :  torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "original_tensor = torch.arange(24).reshape(12, 2) # get a 12x2 tensor \n",
    "print(\"Original : \\n\", original_tensor)\n",
    "print(\"Original Shape : \", original_tensor.shape)\n",
    "\n",
    "transposed_tensor = original_tensor.transpose(0,1) # specify the index to transpose, in terms of dimension, we transpose rows and columns \n",
    "print(\"Transposed : \\n\", transposed_tensor)\n",
    "print(\"Transposed Shape : \", transposed_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : \n",
      " tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "Original Shape :  torch.Size([2, 3, 4])\n",
      "Permuted : \n",
      " tensor([[[ 0,  4,  8],\n",
      "         [12, 16, 20]],\n",
      "\n",
      "        [[ 1,  5,  9],\n",
      "         [13, 17, 21]],\n",
      "\n",
      "        [[ 2,  6, 10],\n",
      "         [14, 18, 22]],\n",
      "\n",
      "        [[ 3,  7, 11],\n",
      "         [15, 19, 23]]])\n",
      "Permuted Shape :  torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Permute operation to rearrange dimesnion \n",
    "original_tensor = torch.arange(24).reshape(2, 3, 4) # get a 2x3x4 tensor \n",
    "print(\"Original : \\n\", original_tensor)\n",
    "print(\"Original Shape : \", original_tensor.shape)\n",
    "\n",
    "# from original tensor 2x3x4 (two tensors 3x4 (0, 1, 2)...\n",
    "# how to rearrange it 4x2x3 (2, 0, 1) to replace indces order (4 tensor of dim 2x3)\n",
    "permuted_tensor = original_tensor.permute(2, 0, 1) # we specify the index of permutation for new dimensions \n",
    "print(\"Permuted : \\n\", permuted_tensor)\n",
    "print(\"Permuted Shape : \", permuted_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f56f0c",
   "metadata": {},
   "source": [
    "## Cloning and Detaching \n",
    "\n",
    "similarly to numpy copy to create a copy of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e147bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# with standard python code \n",
    "a = [1, 2, 3]\n",
    "b = a # both a and b are same entity \n",
    "\n",
    "# mentioning b to hold address of a...\n",
    "import copy\n",
    "b = copy.deepcopy(a)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# separate copy, holding a difference reference in memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clone a pytorch tensor \n",
    "original_tensor = torch.ones(3, 3, requires_grad=True) # specify to pytorch that this tensor will be used for gradient \n",
    "# grad is part of computation graph (original tensor on computation graph )\n",
    "\n",
    "copy_tensor = original_tensor.clone() # new clone indipendent of the original one, same value but different memory allocation on RAM \n",
    "# copy part of computation graph \n",
    "\n",
    "detached_tensor = original_tensor.detach() # this detach the original tensor from computation graph \n",
    "# detached is no more part of computation graph, but storage will be same as the original tensor \n",
    "# remove from computatiion graph, but returned tensor share same storage with original one, not influencing copy\n",
    "# doesn't influence the copy one "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
